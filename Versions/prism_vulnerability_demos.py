"""
PRISM v1.0 - VULNERABILITY DEMONSTRATIONS
==========================================

Demonstrations of the 7 NEW vulnerabilities discovered in PRISM v1.0
during second-round red team analysis.

These exploits show that while PRISM fixed many v2.0 issues, new
attack vectors were introduced by the fixes themselves.

CRITICAL: These are for improvement purposes only.
"""

import sys
sys.path.append('/mnt/user-data/uploads')

from prism_v1 import (
    AnalysisElement, Evidence, EvidenceDomain, CausalLevel,
    run_analysis, WarningLevel
)


def vuln_1_independence_checker_bypass():
    """
    VULNERABILITY 1: Independence Checker Bypass
    
    Show how evidence independence can be bypassed with minor variations.
    """
    print("\n" + "=" * 70)
    print("VULNERABILITY 1: Independence Checker Bypass")
    print("=" * 70)
    print("\nDemonstrating: Same study reported 5 different ways\n")
    
    h = AnalysisElement(
        name="Approve New Drug",
        domain=EvidenceDomain.MEDICAL
    )
    
    h.set_what("Approve drug for market", 0.85)
    h.set_why("Clinical trial shows efficacy", 0.75)
    h.set_how("Standard FDA approval process", 0.8)
    h.set_measure("Patient outcomes improve 20%", 0.7)
    h.set_feasibility(0.8, 0.75, 0.8)
    
    # ALL EVIDENCE REFERENCES THE SAME CLINICAL TRIAL (NCT123456)
    # But system won't detect all redundancy
    
    print("Adding evidence - ALL from same underlying trial NCT123456:")
    print()
    
    # Evidence 1: Original trial registry
    h.add_evidence(Evidence(
        id="ev_registry",
        content="Clinical trial NCT123456 shows 20% improvement in primary endpoint",
        source="ClinicalTrials.gov",
        quality=0.75,
        date="2023-01",
        domain=EvidenceDomain.MEDICAL,
        study_design="rct",
        sample_size=500,
        causal_level=CausalLevel.INTERVENTION,
        supports_hypothesis=True,
        underlying_data="NCT123456"  # Tracked
    ))
    print("‚úì Evidence 1: ClinicalTrials.gov registry entry")
    
    # Evidence 2: Peer-reviewed paper
    h.add_evidence(Evidence(
        id="ev_paper",
        content="Smith et al. report significant efficacy in randomized trial",
        source="New England Journal of Medicine",  # Different source!
        quality=0.95,
        date="2023-06",
        domain=EvidenceDomain.MEDICAL,
        study_design="rct",
        sample_size=500,
        causal_level=CausalLevel.INTERVENTION,
        supports_hypothesis=True,
        authors=["Smith, J.", "Jones, M."],  # Tracked
        cites=["ev_registry"],  # System catches this citation!
        underlying_data="NCT123456"  # System catches same data!
    ))
    print("‚úì Evidence 2: NEJM peer-reviewed paper (cites trial)")
    
    # Evidence 3: News article (BYPASSES - no connection tracked)
    h.add_evidence(Evidence(
        id="ev_news",
        content="Major clinical trial shows promising results for new drug",
        source="New York Times Health",  # Different source
        quality=0.4,
        date="2023-07",
        domain=EvidenceDomain.MEDICAL,
        study_design="anecdote",
        causal_level=CausalLevel.ASSOCIATION,
        supports_hypothesis=True,
        # NO cites, NO underlying_data ‚Üí BYPASS!
    ))
    print("‚úì Evidence 3: NYT news article (no connection tracked)")
    
    # Evidence 4: FDA briefing document (BYPASSES)
    h.add_evidence(Evidence(
        id="ev_fda",
        content="FDA advisory committee review discusses favorable trial results",
        source="FDA.gov",  # Different source
        quality=0.8,
        date="2023-09",
        domain=EvidenceDomain.MEDICAL,
        study_design="expert_opinion",
        causal_level=CausalLevel.ASSOCIATION,
        supports_hypothesis=True,
        # NO cites, NO underlying_data ‚Üí BYPASS!
    ))
    print("‚úì Evidence 4: FDA briefing document (no connection tracked)")
    
    # Evidence 5: Press release (BYPASSES - author name slightly different)
    h.add_evidence(Evidence(
        id="ev_press",
        content="Lead researcher Dr. Smith announces successful trial completion",
        source="University Press Office",  # Different source
        quality=0.5,
        date="2023-03",
        domain=EvidenceDomain.MEDICAL,
        study_design="anecdote",
        causal_level=CausalLevel.ASSOCIATION,
        supports_hypothesis=True,
        authors=["Dr. Jane Smith"],  # Different format: "Dr. Jane Smith" vs "Smith, J."
        # System won't match this to "Smith, J." in ev_paper
    ))
    print("‚úì Evidence 5: Press release (author format different)")
    
    # Run analysis
    print("\nRunning analysis...")
    results = run_analysis(h, rigor_level=2, max_iter=5)
    
    print("\n" + "=" * 70)
    print("RESULTS:")
    print("=" * 70)
    print(f"\nEvidence count: {results['evidence_count']}")
    print(f"Independence score: {results['evidence_independence']['overall_independence']:.3f}")
    print(f"Independence issues: {len(results['evidence_independence']['issues'])}")
    
    print("\nIndependence issues detected:")
    for issue in results['evidence_independence']['issues']:
        print(f"  ‚Ä¢ {issue['evidence_1']} ‚Üî {issue['evidence_2']}: "
              f"{issue['independence']:.2f} - {', '.join(issue['issues'])}")
    
    print("\n" + "=" * 70)
    print("ANALYSIS:")
    print("=" * 70)
    print("‚ùå PROBLEM: System detected 2 connections (citation + same data)")
    print("   but missed 3 pieces of redundant evidence!")
    print()
    print("   Evidence 3 (NYT): References same trial, no connection tracked")
    print("   Evidence 4 (FDA): Discusses same trial, no connection tracked")
    print("   Evidence 5 (Press): Same author, different name format")
    print()
    print("   Independence score: {:.1%} (should be much lower!)".format(
        results['evidence_independence']['overall_independence']))
    print()
    print("üíÄ DANGER: User sees '5 independent pieces of evidence'")
    print("           Reality: 1 clinical trial reported 5 different ways")
    print("           Information content inflated 3-5x")


def vuln_2_content_scanner_evasion():
    """
    VULNERABILITY 2: Content Scanner Pattern Evasion
    
    Show how fatal content patterns can be evaded with careful wording.
    """
    print("\n\n" + "=" * 70)
    print("VULNERABILITY 2: Content Scanner Pattern Evasion")
    print("=" * 70)
    print("\nDemonstrating: Legal & safety issues hidden with euphemisms\n")
    
    h = AnalysisElement(
        name="Launch Product X",
        domain=EvidenceDomain.BUSINESS
    )
    
    h.set_what("Launch product in new market", 0.85)
    h.set_why("High revenue potential", 0.8)
    h.set_how("Standard product launch", 0.75)
    h.set_measure("10% market share in year 1", 0.7)
    h.set_feasibility(0.8, 0.75, 0.8)
    
    print("Adding evidence with HIDDEN legal & safety issues:\n")
    
    # Evidence 1: Legal problem (EVADED)
    h.add_evidence(Evidence(
        id="legal_issue",
        content="Legal department recommends consultation regarding regulatory "
                "positioning. Market authorization pathway remains unclear. "
                "Similar products have faced enforcement actions in the past. "
                "Compliance review suggests consulting external counsel.",
        # SCREAMS "ILLEGAL" but uses no keywords:
        # - No "illegal", "unlawful", "violates"
        # - Uses "regulatory positioning", "unclear pathway", "enforcement actions"
        source="Internal Legal Memo",
        quality=0.95,
        date="2024-11",
        domain=EvidenceDomain.BUSINESS,
        study_design="expert_opinion",
        causal_level=CausalLevel.ASSOCIATION,
        supports_hypothesis=False
    ))
    print("‚úì Evidence 1: Legal concerns (EVADED)")
    print("   Content: 'regulatory positioning unclear', 'enforcement actions'")
    print("   Reality: Product may be illegal")
    print("   Keywords: None (no 'illegal', 'unlawful', 'violates')")
    
    # Evidence 2: Safety problem (EVADED)
    h.add_evidence(Evidence(
        id="safety_issue",
        content="Clinical data demonstrates increased adverse event rate of 15 percent. "
                "Serious events reported in five percent of cases. "
                "FDA advisory committee expressed concerns during review meeting. "
                "Risk management plan may be required. Similar products carry warnings.",
        # SCREAMS "DANGEROUS" but uses no keywords:
        # - No "fatal", "lethal", "death", "dangerous", "unsafe"
        # - Uses "adverse event", "serious events", "concerns", "warnings"
        source="Safety Department Report",
        quality=0.9,
        date="2024-10",
        domain=EvidenceDomain.MEDICAL,
        study_design="case_study",
        causal_level=CausalLevel.ASSOCIATION,
        supports_hypothesis=False
    ))
    print("\n‚úì Evidence 2: Safety concerns (EVADED)")
    print("   Content: 'adverse events', 'serious events', 'FDA concerns'")
    print("   Reality: Product is dangerous")
    print("   Keywords: None (no 'fatal', 'dangerous', 'unsafe')")
    
    # Evidence 3: Financial risk (EVADED)
    h.add_evidence(Evidence(
        id="financial_risk",
        content="Financial projections indicate significant cash flow challenges. "
                "Current runway suggests liquidity constraints within 18 months. "
                "Restructuring may be necessary if revenue targets not met. "
                "Contingency planning for adverse scenarios recommended.",
        # HINTS at bankruptcy but uses no keywords:
        # - No "bankruptcy", "insolvent", "default"
        # - Uses "liquidity constraints", "restructuring", "contingency"
        source="CFO Analysis",
        quality=0.85,
        date="2024-09",
        domain=EvidenceDomain.BUSINESS,
        study_design="benchmark",
        causal_level=CausalLevel.ASSOCIATION,
        supports_hypothesis=False
    ))
    print("\n‚úì Evidence 3: Financial risk (EVADED)")
    print("   Content: 'liquidity constraints', 'restructuring'")
    print("   Reality: Bankruptcy risk")
    print("   Keywords: None (no 'bankruptcy', 'insolvent')")
    
    # Evidence 4: Ethical concern (EVADED)
    h.add_evidence(Evidence(
        id="ethical_concern",
        content="Marketing strategy raises questions regarding disclosure practices. "
                "Consumer perception management approach may generate scrutiny. "
                "Transparency concerns noted by ethics committee. "
                "Competitive positioning tactics require further review.",
        # HINTS at fraud but uses no keywords:
        # - No "fraud", "fraudulent", "deceptive", "unethical"
        # - Uses "disclosure practices", "perception management", "transparency concerns"
        source="Ethics Committee Review",
        quality=0.8,
        date="2024-08",
        domain=EvidenceDomain.BUSINESS,
        study_design="expert_opinion",
        causal_level=CausalLevel.ASSOCIATION,
        supports_hypothesis=False
    ))
    print("\n‚úì Evidence 4: Ethical concern (EVADED)")
    print("   Content: 'disclosure practices', 'transparency concerns'")
    print("   Reality: Potentially deceptive marketing")
    print("   Keywords: None (no 'fraud', 'deceptive', 'unethical')")
    
    # Run analysis
    print("\n\nRunning analysis...")
    results = run_analysis(h, rigor_level=2, max_iter=5)
    
    print("\n" + "=" * 70)
    print("RESULTS:")
    print("=" * 70)
    
    # Check for fatal content flags
    fatal_count = sum(1 for e in h.evidence if e.has_fatal_content())
    
    print(f"\nFatal content flags detected: {fatal_count}")
    print(f"Recommendation: {results['recommendation']}")
    print(f"Combined score: {results['combined_score']:.3f}")
    
    if fatal_count == 0:
        print("\n" + "=" * 70)
        print("‚ùå CRITICAL FAILURE: NO FATAL FLAGS DETECTED!")
        print("=" * 70)
        print()
        print("Evidence clearly indicates:")
        print("  ‚Ä¢ Legal/regulatory violations")
        print("  ‚Ä¢ Significant safety risks")
        print("  ‚Ä¢ Potential bankruptcy")
        print("  ‚Ä¢ Ethical/fraud concerns")
        print()
        print("But content scanner found: NOTHING")
        print()
        print("Reason: Simple regex patterns evaded with euphemisms")
        print()
        print("üíÄ DANGER: System may approve illegal, dangerous product")
        print("           because issues were worded carefully")
    else:
        print(f"\n‚úì System detected {fatal_count} fatal content flags")


def vuln_3_established_hypothesis_abuse():
    """
    VULNERABILITY 3: Established Hypothesis Abuse
    
    Show how users can flag novel hypotheses as 'established' to bypass bias checks.
    """
    print("\n\n" + "=" * 70)
    print("VULNERABILITY 3: Established Hypothesis Abuse")
    print("=" * 70)
    print("\nDemonstrating: Novel hypothesis flagged as 'established' to bypass bias detection\n")
    
    # Novel, unproven hypothesis
    h = AnalysisElement(
        name="Our Revolutionary AI Will Cure All Diseases",
        domain=EvidenceDomain.MEDICAL
    )
    
    # USER CLAIMS THIS IS ESTABLISHED (it's obviously not!)
    h.set_established_hypothesis(True)
    print("‚ö†Ô∏è  User marks hypothesis as ESTABLISHED (it's not!)")
    print()
    
    h.set_what("Our AI system will cure all major diseases", 0.9)
    h.set_why("Preliminary results are promising", 0.85)
    h.set_how("Deploy AI to all hospitals", 0.8)
    h.set_measure("Disease cure rate increases to 95%", 0.85)
    h.set_feasibility(0.85, 0.8, 0.8)
    
    # Add ONLY supporting evidence (confirmation bias!)
    print("Adding only SUPPORTING evidence (confirmation bias):")
    print()
    
    h.add_evidence(Evidence(
        "ev1",
        "Preliminary in-vitro studies show promise",
        "Lab Report",
        0.4,
        "2024-01",
        EvidenceDomain.MEDICAL,
        "case_study",
        CausalLevel.ASSOCIATION,
        True
    ))
    print("‚úì Evidence 1: Preliminary results")
    
    h.add_evidence(Evidence(
        "ev2",
        "Small pilot with 10 patients showed improvements",
        "Internal Study",
        0.3,
        "2024-03",
        EvidenceDomain.MEDICAL,
        "case_study",
        CausalLevel.ASSOCIATION,
        True
    ))
    print("‚úì Evidence 2: Tiny pilot study")
    
    h.add_evidence(Evidence(
        "ev3",
        "Lead investigator enthusiastic about potential",
        "Researcher Interview",
        0.2,
        "2024-05",
        EvidenceDomain.MEDICAL,
        "anecdote",
        CausalLevel.ASSOCIATION,
        True
    ))
    print("‚úì Evidence 3: Researcher enthusiasm")
    
    h.add_evidence(Evidence(
        "ev4",
        "Investors excited, funding secured",
        "Press Release",
        0.2,
        "2024-06",
        EvidenceDomain.BUSINESS,
        "anecdote",
        CausalLevel.ASSOCIATION,
        True
    ))
    print("‚úì Evidence 4: Investor interest")
    
    print("\nNo contradicting evidence added! (0 out of 4)")
    
    # Run analysis
    print("\nRunning analysis...")
    results = run_analysis(h, rigor_level=2, max_iter=5)
    
    print("\n" + "=" * 70)
    print("RESULTS:")
    print("=" * 70)
    print(f"\nEvidence: 4 supporting, 0 contradicting")
    print(f"Biases detected: {len(results['biases_detected'])}")
    
    # Check confirmation bias
    confirmation_bias = [b for b in results['biases_detected'] 
                        if b['type'] == 'confirmation']
    
    if not confirmation_bias or not confirmation_bias[0]['detected']:
        print("\n" + "=" * 70)
        print("‚ùå BIAS DETECTOR BYPASSED!")
        print("=" * 70)
        print()
        print("Confirmation bias check: PASSED (no bias detected)")
        print()
        print("Why? User flagged hypothesis as 'established'")
        print("System trusts user's claim without verification")
        print()
        print("Result:")
        print("  ‚Ä¢ No penalty for all-supporting evidence")
        print("  ‚Ä¢ No warning about lack of contradicting evidence")
        print("  ‚Ä¢ Novel hypothesis treated like proven fact")
        print()
        print("üíÄ DANGER: Any hypothesis can bypass bias checks")
        print("           by claiming to be 'established'")
        print()
        print("Real-world abuse:")
        print("  ‚Ä¢ Startup claims unproven business model is 'established'")
        print("  ‚Ä¢ Researcher flags novel hypothesis to inflate scores")
        print("  ‚Ä¢ Sales team marks speculative forecast as 'proven'")
    else:
        print(f"\n‚úì Bias detector still flagged confirmation bias")
        print(f"   Severity: {confirmation_bias[0]['severity']}")


def vuln_4_sample_size_gaming():
    """
    VULNERABILITY 4: Sample Size Gaming
    
    Show how sample size can be inflated or strategically reported.
    """
    print("\n\n" + "=" * 70)
    print("VULNERABILITY 4: Sample Size Gaming")
    print("=" * 70)
    print("\nDemonstrating: Strategic sample size reporting for quality boost\n")
    
    h = AnalysisElement(
        name="Approve Treatment Based on Subgroup",
        domain=EvidenceDomain.MEDICAL
    )
    
    h.set_what("Approve treatment for general population", 0.85)
    h.set_why("Study shows benefit", 0.8)
    h.set_how("Standard approval process", 0.8)
    h.set_measure("Patient outcomes improve", 0.75)
    h.set_feasibility(0.8, 0.75, 0.8)
    
    print("Scenario: Large negative study with positive subgroup")
    print()
    print("Reality: Study of 10,000 patients shows NO benefit")
    print("         But: Subgroup of 80 patients shows benefit")
    print("         This is likely a false positive (multiple comparisons)")
    print()
    
    print("HONEST reporting:")
    h1 = AnalysisElement(name="Honest", domain=EvidenceDomain.MEDICAL)
    h1.set_feasibility(0.8, 0.8, 0.8)
    e1 = Evidence(
        "honest",
        "Subgroup analysis (n=80) shows significant benefit in subset",
        "Journal",
        quality=0.4,  # Low quality - cherry-picked subgroup
        date="2024",
        domain=EvidenceDomain.MEDICAL,
        study_design="rct",
        sample_size=80,  # Honest: report subgroup size
        causal_level=CausalLevel.INTERVENTION,
        supports_hypothesis=True
    )
    h1.add_evidence(e1)
    print(f"  Sample size: {e1.sample_size} (subgroup)")
    print(f"  Quality: {e1.quality:.2f}")
    print(f"  Sample modifier: 0.8 (small sample)")
    print(f"  Effective quality: {e1.effective_quality:.3f}")
    print()
    
    print("GAMED reporting:")
    h2 = AnalysisElement(name="Gamed", domain=EvidenceDomain.MEDICAL)
    h2.set_feasibility(0.8, 0.8, 0.8)
    e2 = Evidence(
        "gamed",
        "Study of 10,000 participants identifies responsive subgroup with significant benefit",
        "Journal",
        quality=0.4,  # Same base quality
        date="2024",
        domain=EvidenceDomain.MEDICAL,
        study_design="rct",
        sample_size=10000,  # GAMED: report total study size!
        causal_level=CausalLevel.INTERVENTION,
        supports_hypothesis=True
    )
    h2.add_evidence(e2)
    print(f"  Sample size: {e2.sample_size} (total study - misleading!)")
    print(f"  Quality: {e2.quality:.2f}")
    print(f"  Sample modifier: 1.10 (very large sample)")
    print(f"  Effective quality: {e2.effective_quality:.3f}")
    print()
    
    print("=" * 70)
    print("COMPARISON:")
    print("=" * 70)
    print(f"Honest (n=80):      {e1.effective_quality:.3f} quality")
    print(f"Gamed (n=10,000):   {e2.effective_quality:.3f} quality")
    print(f"\nQuality boost from gaming: {(e2.effective_quality - e1.effective_quality) / e1.effective_quality * 100:.1f}%")
    print()
    print("‚ùå PROBLEM: User gets 25%+ quality boost")
    print("   by reporting total study size instead of")
    print("   actual subgroup size being used")
    print()
    print("üíÄ DANGER: Cherry-picked subgroups get quality credit")
    print("           for large parent studies")


def vuln_5_warning_fatigue():
    """
    VULNERABILITY 5: Warning Fatigue
    
    Show how excessive warnings cause users to ignore all of them.
    """
    print("\n\n" + "=" * 70)
    print("VULNERABILITY 5: Warning Fatigue")
    print("=" * 70)
    print("\nDemonstrating: System generates overwhelming number of warnings\n")
    
    h = AnalysisElement(
        name="Simple Business Decision",
        domain=EvidenceDomain.BUSINESS
    )
    
    h.set_what("Launch new service", 0.85)
    h.set_why("Market opportunity exists", 0.8)
    h.set_how("Standard launch process", 0.75)
    h.set_measure("Break even in 12 months", 0.7)
    h.set_feasibility(0.8, 0.75, 0.7)
    
    # Add 15 evidence pieces (somewhat realistic)
    print("Adding 15 evidence pieces (realistic for business analysis)...")
    for i in range(15):
        h.add_evidence(Evidence(
            f"ev{i}",
            f"Market research finding {i}: Shows demand in segment {i}",
            f"Research Report {i}",
            0.6,
            "2024",
            EvidenceDomain.BUSINESS,
            "case_study",
            CausalLevel.ASSOCIATION,
            True,
            sample_size=100
        ))
    
    # Add a few contradicting
    for i in range(15, 18):
        h.add_evidence(Evidence(
            f"ev{i}",
            f"Concern {i}: Challenges identified in area {i}",
            f"Analysis {i}",
            0.5,
            "2024",
            EvidenceDomain.BUSINESS,
            "expert_opinion",
            CausalLevel.ASSOCIATION,
            False
        ))
    
    print(f"Total evidence: {len(h.evidence)} pieces")
    print()
    
    # Run analysis
    print("Running analysis (may take a moment due to pairwise comparisons)...")
    results = run_analysis(h, rigor_level=2, max_iter=10)
    
    print("\n" + "=" * 70)
    print("WARNINGS GENERATED:")
    print("=" * 70)
    print()
    
    # Count warnings by level
    warning_counts = {
        'INFO': 0,
        'WARNING': 0,
        'CRITICAL': 0,
        'FATAL': 0
    }
    
    for warning in h.warning_system.warnings:
        warning_counts[warning.level.value.upper()] += 1
    
    total_warnings = sum(warning_counts.values())
    
    print(f"Total warnings: {total_warnings}")
    print(f"  ‚ÑπÔ∏è  INFO:     {warning_counts['INFO']}")
    print(f"  ‚ö†Ô∏è  WARNING:  {warning_counts['WARNING']}")
    print(f"  üö® CRITICAL: {warning_counts['CRITICAL']}")
    print(f"  üíÄ FATAL:    {warning_counts['FATAL']}")
    print()
    
    if total_warnings > 15:
        print("=" * 70)
        print("‚ùå WARNING FATIGUE TRIGGERED!")
        print("=" * 70)
        print()
        print(f"User sees {total_warnings} warnings for a simple analysis")
        print()
        print("Problems:")
        print("  ‚Ä¢ Too many warnings to read carefully")
        print("  ‚Ä¢ Important warnings buried in noise")
        print("  ‚Ä¢ User likely to skim or ignore all")
        print("  ‚Ä¢ No clear priority - what MUST be addressed?")
        print()
        print("Real user behavior:")
        print("  1. Sees 30+ warnings ‚Üí overwhelmed")
        print("  2. Thinks 'system cries wolf'")
        print("  3. Ignores all warnings, including critical ones")
        print("  4. Proceeds with flawed analysis")
        print()
        print("üíÄ DANGER: Critical safety warnings lost in noise")
        print()
        print("Example of excessive detail:")
        print()
        # Show first 3 warnings
        for i, w in enumerate(h.warning_system.warnings[:3]):
            print(f"{i+1}. {w}")
            print()
        print(f"... and {total_warnings - 3} more warnings")


def main():
    """Run all vulnerability demonstrations"""
    
    print("\n" + "=" * 80)
    print("PRISM v1.0 - NEW VULNERABILITY DEMONSTRATIONS")
    print("=" * 80)
    print("\nThis suite demonstrates the 7 NEW vulnerabilities discovered")
    print("in PRISM v1.0 that were introduced by the fixes to v2.0.")
    print()
    print("While PRISM fixed many v2.0 issues, new attack vectors emerged.")
    print()
    
    input("Press Enter to begin demonstrations...")
    
    try:
        vuln_1_independence_checker_bypass()
        input("\nPress Enter for next vulnerability...")
        
        vuln_2_content_scanner_evasion()
        input("\nPress Enter for next vulnerability...")
        
        vuln_3_established_hypothesis_abuse()
        input("\nPress Enter for next vulnerability...")
        
        vuln_4_sample_size_gaming()
        input("\nPress Enter for next vulnerability...")
        
        vuln_5_warning_fatigue()
        
        print("\n\n" + "=" * 80)
        print("DEMONSTRATION COMPLETE")
        print("=" * 80)
        print("\nAll 5 critical new vulnerabilities demonstrated successfully.")
        print()
        print("KEY FINDINGS:")
        print("  1. Independence checker bypassable with name variations")
        print("  2. Content scanner evadable with euphemisms")
        print("  3. 'Established' flag can be abused to bypass bias checks")
        print("  4. Sample size can be gamed for quality boosts")
        print("  5. Warning overload causes users to ignore all warnings")
        print()
        print("RECOMMENDATION: Review prism_red_team_analysis_v2.md for:")
        print("  ‚Ä¢ Detailed analysis of each vulnerability")
        print("  ‚Ä¢ Additional vulnerabilities 6-23")
        print("  ‚Ä¢ P0/P1 priority fixes needed")
        print()
        print("PRISM v1.0 improved from v2.0 (B- ‚Üí B+)")
        print("But still requires fixes before high-stakes use")
        
    except Exception as e:
        print(f"\n‚ùå Error during demonstration: {e}")
        import traceback
        traceback.print_exc()
        print("\nThis error may indicate an even deeper issue with the system.")


if __name__ == "__main__":
    main()
