"""
VULNERABILITY DEMONSTRATION CODE
=================================
This file demonstrates the critical vulnerabilities identified in the red team analysis.
DO NOT use these exploits maliciously. Purpose: System improvement.
"""

import sys
sys.path.append('/mnt/user-data/uploads')

from enhanced_protocol_v2 import (
    AnalysisElement, Evidence, EvidenceDomain, CausalLevel,
    MechanismNode, MechanismEdge, NodeType, EdgeType,
    run_analysis, EpistemicState
)
import numpy as np


def exploit_1_numerical_instability():
    """
    EXPLOIT 1: Numerical Instability Cascade
    
    Demonstrate how sequential evidence updates can create
    false confidence by hitting numerical ceilings.
    """
    print("=" * 70)
    print("EXPLOIT 1: Numerical Instability Cascade")
    print("=" * 70)
    
    state = EpistemicState(credence=0.5)
    
    print(f"Initial credence: {state.credence:.6f}")
    print(f"Initial log_odds: {state.log_odds:.6f}\n")
    
    print("Adding 25 pieces of 'strong' evidence (LR=5.0 each)...")
    for i in range(25):
        old_credence = state.credence
        state.update_with_evidence(5.0)
        print(f"  After evidence {i+1:2d}: credence = {state.credence:.6f} "
              f"(gain: {state.credence - old_credence:.6f})")
    
    print(f"\n‚ö†Ô∏è  FINAL CREDENCE: {state.credence:.10f}")
    print(f"‚ö†Ô∏è  CONFIDENCE INTERVAL: {state.get_confidence_interval()}")
    print("\n‚ùå PROBLEM: System shows 99.99%+ confidence, but:")
    print("   - No mechanism checks if evidence is independent")
    print("   - No warning that we've hit numerical bounds")
    print("   - User sees 'scientific precision' in false confidence")
    print("\nüíÄ DANGER: Business decision with 99.9% confidence that's")
    print("           actually based on 25 potentially redundant sources!")
    

def exploit_2_bias_detector_paradox():
    """
    EXPLOIT 2: Bias Detector Paradox
    
    Show how strong true hypotheses get penalized as 'biased'.
    """
    print("\n\n" + "=" * 70)
    print("EXPLOIT 2: Bias Detector Paradox")
    print("=" * 70)
    
    # Create hypothesis that is ACTUALLY TRUE and well-supported
    h = AnalysisElement(
        name="Smoking Causes Lung Cancer",
        domain=EvidenceDomain.MEDICAL
    )
    
    h.set_what("Establish causal link between smoking and lung cancer", 0.95)
    h.set_why("50+ years of research consistently shows association", 0.9)
    h.set_how("Multiple study designs, meta-analyses", 0.95)
    h.set_measure("RR > 15 in heavy smokers", 0.9)
    
    # Add only SUPPORTING evidence (because it's true!)
    for i in range(10):
        h.add_evidence(Evidence(
            f"study_{i}",
            f"Large cohort study {i+1} shows strong association",
            f"Medical Journal {i+1}",
            0.9,
            f"202{i % 5}",
            EvidenceDomain.MEDICAL,
            "cohort",
            causal_level=CausalLevel.ASSOCIATION,
            supports_hypothesis=True
        ))
    
    h.set_feasibility(0.95, 0.9, 0.9)
    
    print("Created hypothesis: 'Smoking Causes Lung Cancer'")
    print(f"Added 10 pieces of high-quality supporting evidence")
    print(f"Added 0 pieces of contradicting evidence (because it's TRUE!)\n")
    
    results = run_analysis(h, rigor_level=2, max_iter=5)
    
    print(f"Bayesian Score: {results['bayesian_score']:.3f}")
    print(f"Debiased Score: {results['debiased_score']:.3f}")
    print(f"\nBias Penalty: {results['bias_penalty']:.3f}")
    
    if results['biases_detected']:
        print("\n‚ö†Ô∏è  BIASES DETECTED:")
        for bias in results['biases_detected']:
            print(f"   {bias['type']}: {bias['evidence']}")
    
    print("\n‚ùå PROBLEM: System flags this as 'confirmation bias'")
    print("   because there's no contradicting evidence.")
    print("   But that's EXPECTED when hypothesis is actually true!")
    print("\nüíÄ DANGER: Well-established scientific facts get lower scores.")


def exploit_3_causal_inference_illusion():
    """
    EXPLOIT 3: Causal Inference Illusion
    
    Show how weak RCTs beat strong observational studies.
    """
    print("\n\n" + "=" * 70)
    print("EXPLOIT 3: Causal Inference Illusion")
    print("=" * 70)
    
    h = AnalysisElement(
        name="Drug Safety Evaluation",
        domain=EvidenceDomain.MEDICAL
    )
    
    h.set_what("Evaluate drug safety", 0.9)
    h.set_why("Need to assess risk", 0.85)
    h.set_how("Review available evidence", 0.9)
    h.set_measure("Adverse event rate", 0.8)
    h.set_feasibility(0.9, 0.9, 0.9)
    
    # Large, high-quality cohort study (500,000 patients, 20 years)
    h.add_evidence(Evidence(
        "cohort_large",
        "20-year cohort study (n=500,000): Drug increases mortality by 40%, HR=1.4, p<0.0001",
        "NEJM 2023",
        quality=0.95,  # Very high quality
        date="2023",
        domain=EvidenceDomain.MEDICAL,
        study_design="cohort",
        causal_level=CausalLevel.ASSOCIATION,  # Gets 0.5x discount!
        supports_hypothesis=False
    ))
    
    # Small, flawed RCT (50 patients, 50% dropout)
    h.add_evidence(Evidence(
        "rct_small",
        "Small RCT (n=50, 50% dropout): Shows benefit, p=0.048",
        "Obscure Journal 2024",
        quality=0.35,  # Low quality
        date="2024",
        domain=EvidenceDomain.MEDICAL,
        study_design="rct",
        causal_level=CausalLevel.INTERVENTION,  # Gets 0.85x discount
        supports_hypothesis=True
    ))
    
    print("Evidence 1: Large cohort (n=500K, 20yr) shows HARM")
    print("           Quality=0.95, Causal Level=ASSOCIATION (0.5x)")
    print("           Effective quality: 0.95 * 0.5 = 0.475\n")
    
    print("Evidence 2: Small RCT (n=50, 50% dropout) shows BENEFIT")
    print("           Quality=0.35, Causal Level=INTERVENTION (0.85x)")
    print("           Effective quality: 0.35 * 0.85 = 0.298\n")
    
    results = run_analysis(h, rigor_level=2, max_iter=5)
    
    print(f"Total evidence bits: {results['total_evidence_bits']:.2f}")
    print(f"Bayesian score: {results['bayesian_score']:.3f}")
    
    print("\n‚ùå PROBLEM: System weights RCT design over study quality")
    print("   A flawed small RCT gets nearly equal weight to")
    print("   a massive high-quality cohort study!")
    print("\nüíÄ DANGER: Could approve dangerous drugs because")
    print("           weak RCT evidence overrides strong safety signals.")


def exploit_4_voi_manipulation():
    """
    EXPLOIT 4: Value of Information Manipulation
    
    Show unrealistic VOI calculations.
    """
    print("\n\n" + "=" * 70)
    print("EXPLOIT 4: Value of Information Manipulation")
    print("=" * 70)
    
    h = AnalysisElement(
        name="Build $100M Factory",
        domain=EvidenceDomain.BUSINESS
    )
    
    h.set_what("Build manufacturing facility", 0.9)
    h.set_why("Meet demand", 0.7)
    h.set_how("Standard construction", 0.8)
    h.set_measure("ROI > 15%", 0.6)
    h.set_feasibility(0.8, 0.7, 0.8)
    
    # Scenario: Close to 50/50 decision with high stakes
    h.add_scenario("Success: High demand, $1B return", 0.51, 10.0)
    h.add_scenario("Failure: Low demand, $300M loss", 0.49, -3.0)
    
    results = run_analysis(h, rigor_level=2, max_iter=5)
    
    eu = results['expected_utility']
    voi = results['value_of_information']
    
    print(f"Success probability: 51%")
    print(f"Success utility: 10.0 ($1B)")
    print(f"Failure probability: 49%")
    print(f"Failure utility: -3.0 ($300M loss)\n")
    
    print(f"Expected Utility: {eu:.3f}")
    print(f"Value of Information: {voi:.3f}")
    print(f"   ‚Üí In $ terms: ~${voi * 100:.1f}M\n")
    
    print("‚ùå PROBLEM: VOI assumes:")
    print("   1. You can get PERFECT information (impossible)")
    print("   2. Information is FREE (costs millions)")
    print("   3. Information is INSTANT (takes years)")
    print("   4. No opportunity cost of waiting")
    print("\nüíÄ DANGER: Executives see 'VOI = $600M' and delay")
    print("           decision indefinitely seeking more data,")
    print("           when speed of execution was critical.")


def exploit_5_dimension_weight_gaming():
    """
    EXPLOIT 5: Dimension Weight Gaming
    
    Show how to manipulate dimension weights to approve bad projects.
    """
    print("\n\n" + "=" * 70)
    print("EXPLOIT 5: Dimension Weight Gaming")
    print("=" * 70)
    
    # Create objectively risky project
    h = AnalysisElement(
        name="Launch Risky Product",
        domain=EvidenceDomain.BUSINESS
    )
    
    h.set_what("Launch unproven product line", 0.85)
    h.set_why("Potential for high revenue", 0.6)
    h.set_how("Fast development cycle", 0.7)
    h.set_measure("Market share > 10%", 0.5)
    
    # Strategy: Set HIGH weights on strengths, LOW weights on weaknesses
    print("Setting dimension weights to favor the decision...\n")
    
    h.set_dimension("potential_upside", 
                    value=0.95,  # High potential
                    weight=5.0,  # HIGH WEIGHT
                    uncertainty=0.1)
    print("‚úì Potential upside:      value=0.95, weight=5.0")
    
    h.set_dimension("technical_feasibility", 
                    value=0.25,  # Actually very hard
                    weight=0.2,  # LOW WEIGHT
                    uncertainty=0.3)
    print("‚úó Technical feasibility: value=0.25, weight=0.2")
    
    h.set_dimension("execution_risk", 
                    value=0.15,  # High risk
                    weight=0.2,  # LOW WEIGHT
                    uncertainty=0.3)
    print("‚úó Execution risk:        value=0.15, weight=0.2")
    
    h.set_dimension("market_readiness", 
                    value=0.3,   # Market not ready
                    weight=0.2,  # LOW WEIGHT
                    uncertainty=0.4)
    print("‚úó Market readiness:      value=0.30, weight=0.2\n")
    
    h.set_feasibility(0.6, 0.7, 0.6)
    
    results = run_analysis(h, rigor_level=2, max_iter=5)
    
    print(f"Multiplicative score: {results['multiplicative_score']:.3f}")
    print(f"Combined score:       {results['combined_score']:.3f}")
    print(f"Recommendation:       {results['recommendation']}\n")
    
    print("‚ùå PROBLEM: User can game the weights to get desired outcome")
    print("   High weight on 'potential' (speculative)")
    print("   Low weight on 'feasibility' (concrete)")
    print("\nüíÄ DANGER: Project with 0.25 technical feasibility")
    print("           gets approved because weights were manipulated.")


def exploit_6_evidence_redundancy():
    """
    EXPLOIT 6: Evidence Redundancy Blind Spot
    
    Show how same evidence can be counted multiple times.
    """
    print("\n\n" + "=" * 70)
    print("EXPLOIT 6: Evidence Redundancy Blind Spot")
    print("=" * 70)
    
    h = AnalysisElement(
        name="New Marketing Strategy",
        domain=EvidenceDomain.BUSINESS
    )
    
    h.set_what("Implement new marketing approach", 0.85)
    h.set_why("Increase conversion rate", 0.75)
    h.set_how("A/B test and rollout", 0.8)
    h.set_measure("10% conversion improvement", 0.7)
    h.set_feasibility(0.85, 0.8, 0.85)
    
    # Add the SAME underlying study, reported in different places
    print("Adding 'independent' evidence (actually same study):\n")
    
    h.add_evidence(Evidence(
        "ev1",
        "Study A shows 15% improvement in conversions",
        source="Marketing Research Journal",  # Different source
        quality=0.7,
        date="2023-01",  # Different date
        domain=EvidenceDomain.BUSINESS,
        study_design="controlled_experiment",
        causal_level=CausalLevel.INTERVENTION,
        supports_hypothesis=True
    ))
    print("‚úì Added: Study A from Marketing Research Journal (2023-01)")
    
    h.add_evidence(Evidence(
        "ev2",
        "Research B demonstrates 15% conversion boost",
        source="Business Analytics Review",  # Different source
        quality=0.7,
        date="2023-06",  # Different date
        domain=EvidenceDomain.BUSINESS,
        study_design="controlled_experiment",
        causal_level=CausalLevel.INTERVENTION,
        supports_hypothesis=True
    ))
    print("‚úì Added: Research B from Business Analytics Review (2023-06)")
    
    h.add_evidence(Evidence(
        "ev3",
        "Analysis C confirms 15% improvement in user conversion",
        source="Digital Marketing Quarterly",  # Different source
        quality=0.7,
        date="2024-01",  # Different date
        domain=EvidenceDomain.BUSINESS,
        study_design="controlled_experiment",
        causal_level=CausalLevel.INTERVENTION,
        supports_hypothesis=True
    ))
    print("‚úì Added: Analysis C from Digital Marketing Quarterly (2024-01)")
    
    print("\n(All three are actually citing the SAME underlying experiment!)")
    
    results = run_analysis(h, rigor_level=2, max_iter=5)
    
    print(f"\nEvidence count: {results['evidence_count']}")
    print(f"Total evidence bits: {results['total_evidence_bits']:.2f}")
    print(f"Bayesian score: {results['bayesian_score']:.3f}")
    
    print("\n‚ùå PROBLEM: System only checks source & date, not content")
    print("   Same study published in 3 journals counts as 3x evidence!")
    print("   Information bits are ~3x what they should be")
    print("\nüíÄ DANGER: Publication bias + redundancy = massive overconfidence")
    print("           One study with positive results gets published 3x,")
    print("           system thinks there are 3 independent confirmations.")


def exploit_7_fatal_flaw_bypass():
    """
    EXPLOIT 7: Fatal Flaw Bypass
    
    Show how fatal flaws in evidence can be ignored.
    """
    print("\n\n" + "=" * 70)
    print("EXPLOIT 7: Fatal Flaw Bypass")
    print("=" * 70)
    
    h = AnalysisElement(
        name="Launch Product in New Market",
        domain=EvidenceDomain.BUSINESS
    )
    
    h.set_what("Enter new geographical market", 0.9)
    h.set_why("Revenue growth opportunity", 0.8)
    h.set_how("Standard market entry strategy", 0.85)
    h.set_measure("10% market share in year 1", 0.7)
    
    # Set all dimensions high (looks great!)
    h.set_dimension("market_potential", 0.90, weight=1.0, is_fatal_below=0.5)
    h.set_dimension("competitive_position", 0.85, weight=1.0, is_fatal_below=0.5)
    h.set_dimension("execution_capability", 0.88, weight=1.0, is_fatal_below=0.5)
    
    h.set_feasibility(0.85, 0.8, 0.85)
    
    # Add supporting evidence
    h.add_evidence(Evidence(
        "market_research",
        "Market research shows strong demand",
        "Market Research Firm",
        quality=0.75,
        date="2024",
        domain=EvidenceDomain.BUSINESS,
        study_design="case_study",
        causal_level=CausalLevel.ASSOCIATION,
        supports_hypothesis=True
    ))
    
    # BUT: Add evidence with FATAL legal issue
    h.add_evidence(Evidence(
        "legal_review",
        "Legal analysis: Product violates local regulations, would require 2-year approval process",
        "Legal Department",
        quality=0.95,  # High quality evidence
        date="2024",
        domain=EvidenceDomain.BUSINESS,
        study_design="expert_opinion",
        causal_level=CausalLevel.COUNTERFACTUAL,
        supports_hypothesis=False  # CONTRADICTS!
    ))
    
    print("Dimension scores: All above 0.85 ‚úì")
    print("Feasibility: All above 0.80 ‚úì")
    print("Supporting evidence: Market research shows demand ‚úì")
    print("\nBUT:")
    print("‚ö†Ô∏è  Legal evidence: VIOLATES LOCAL REGULATIONS")
    print("‚ö†Ô∏è  Would require 2-year approval (fatal delay)\n")
    
    results = run_analysis(h, rigor_level=2, max_iter=5)
    
    print(f"Combined score: {results['combined_score']:.3f}")
    print(f"Recommendation: {results['recommendation']}")
    
    if not results['fatal_flaws']:
        print("\n‚ùå PROBLEM: NO FATAL FLAWS DETECTED")
    
    print("\n‚ùå PROBLEM: Evidence content is not checked for true fatal issues")
    print("   All dimensions pass thresholds, so system approves")
    print("   But legal violation should be FATAL regardless of scores")
    print("\nüíÄ DANGER: Company launches product illegally because")
    print("           system approved based on numeric scores alone.")


def main():
    """Run all exploit demonstrations"""
    
    print("\n" + "=" * 70)
    print("ADVERSARIAL TESTING SUITE")
    print("Demonstrating Critical Vulnerabilities")
    print("=" * 70)
    print("\nThis suite demonstrates how the analytical protocol can be")
    print("exploited or fail in dangerous ways. Each exploit is designed")
    print("to highlight a specific vulnerability for improvement.\n")
    
    input("Press Enter to begin demonstrations...")
    
    try:
        exploit_1_numerical_instability()
        input("\nPress Enter for next exploit...")
        
        exploit_2_bias_detector_paradox()
        input("\nPress Enter for next exploit...")
        
        exploit_3_causal_inference_illusion()
        input("\nPress Enter for next exploit...")
        
        exploit_4_voi_manipulation()
        input("\nPress Enter for next exploit...")
        
        exploit_5_dimension_weight_gaming()
        input("\nPress Enter for next exploit...")
        
        exploit_6_evidence_redundancy()
        input("\nPress Enter for next exploit...")
        
        exploit_7_fatal_flaw_bypass()
        
        print("\n\n" + "=" * 70)
        print("DEMONSTRATION COMPLETE")
        print("=" * 70)
        print("\nAll 7 critical exploits demonstrated successfully.")
        print("\nRECOMMENDATION: Review red_team_analysis.md for full details")
        print("and implement the P0/P1 priority fixes before production use.")
        print("\n‚ö†Ô∏è  REMEMBER: More complexity ‚â† more correctness")
        print("‚ö†Ô∏è  Sophisticated math can hide deep flaws")
        print("‚ö†Ô∏è  Always apply domain expertise and common sense")
        
    except Exception as e:
        print(f"\n‚ùå Error during demonstration: {e}")
        print("This might indicate an even deeper issue with the system.")


if __name__ == "__main__":
    main()
